{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williambankes/reinforcement_learning/blob/master/Coursework%2C_part_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part I (20 pts total)\n",
        "---\n",
        "\n",
        "**Name:** Your Name\n",
        "\n",
        "**SN:** Your Student Number\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *22nd March, 2022,*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part1.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "**Context**\n",
        "\n",
        "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
        "\n",
        "**Background reading**\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 1 to 6\n",
        "* Lecture slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "**Overview of this assignment**\n",
        "\n",
        "A) You will use Python to implement several bandit algorithms.\n",
        "\n",
        "B) You will then run these algorithms on a multi-armed Bernoulli bandit problem, and answer question about their empirical performance.\n",
        "\n",
        "C) You will then be asked to reason about the behaviour of different algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run each of the cells below, until you reach the next section **Basic Agents**. You do not have to read or understand the code in the **Setup** section.  After running the cells, feel free to fold away the **Setup** section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "outputs": [],
      "source": [
        "# Import Useful Libraries\n",
        "\n",
        "import collections\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "outputs": [],
      "source": [
        "class BernoulliBandit(object):\n",
        "  \"\"\"A stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities, success_reward=1., fail_reward=0.):\n",
        "    \"\"\"Constructor of a stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    return reward\n",
        "\n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "\n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYxNiGcRxbd0"
      },
      "outputs": [],
      "source": [
        "class NonStationaryBandit(object):\n",
        "  \"\"\"A non-stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities,\n",
        "               success_reward=1., fail_reward=0., change_point=800,\n",
        "               change_is_good=True):\n",
        "    \"\"\"Constructor of a non-stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "      change_point: The number of steps before the rewards change.\n",
        "      change_is_good: Whether the rewards go up (if True), or flip (if False).\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "    self._change_point = change_point\n",
        "    self._change_is_good = change_is_good\n",
        "    self._number_of_steps_so_far = 0\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    self._number_of_steps_so_far += 1\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    \n",
        "    if self._number_of_steps_so_far == self._change_point:\n",
        "      # After some number of steps, the rewards are inverted\n",
        "      #\n",
        "      #  ``The past was alterable. The past never had been altered. Oceania was\n",
        "      #    at war with Eastasia. Oceania had always been at war with Eastasia.``\n",
        "      #            - 1984, Orwell (1949).\n",
        "      reward_dif = (self._s - self._f)\n",
        "      if self._change_is_good:\n",
        "        self._f = self._s + reward_dif\n",
        "      else:\n",
        "        self._s -= reward_dif\n",
        "        self._f += reward_dif\n",
        "      \n",
        "      # Recompute expected values when the rewards change\n",
        "      ps = np.array(self._probs)\n",
        "      self._values = ps * self._s + (1 - ps) * self._f\n",
        "\n",
        "    return reward\n",
        "  \n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "  \n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU7KGFJ0DN-H"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "def smooth(array, smoothing_horizon=100., initial_value=0.):\n",
        "  \"\"\"Smoothing function for plotting.\"\"\"\n",
        "  smoothed_array = []\n",
        "  value = initial_value\n",
        "  b = 1./smoothing_horizon\n",
        "  m = 1.\n",
        "  for x in array:\n",
        "    m *= 1. - b\n",
        "    lr = b/(1 - m)\n",
        "    value += lr*(x - value)\n",
        "    smoothed_array.append(value)\n",
        "  return np.array(smoothed_array)\n",
        "\n",
        "def plot(algs, plot_data, repetitions=30):\n",
        "  \"\"\"Plot results of a bandit experiment.\"\"\"\n",
        "  algs_per_row = 4\n",
        "  n_algs = len(algs)\n",
        "  n_rows = (n_algs - 2)//algs_per_row + 1\n",
        "  fig = plt.figure(figsize=(10, 4*n_rows))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.35)\n",
        "  clrs = ['#000000', '#00bb88', '#0033ff', '#aa3399', '#ff6600']\n",
        "  lss = ['--', '-', '-', '-', '-']\n",
        "  for i, p in enumerate(plot_data):\n",
        "    for c in range(n_rows):\n",
        "      ax = fig.add_subplot(n_rows, len(plot_data), i + 1 + c*len(plot_data))\n",
        "      ax.grid(0)\n",
        "\n",
        "      current_algs = [algs[0]] + algs[c*algs_per_row + 1:(c + 1)*algs_per_row + 1]\n",
        "      for alg, clr, ls in zip(current_algs, clrs, lss):\n",
        "        data = p.data[alg.name]\n",
        "        m = smooth(np.mean(data, axis=0))\n",
        "        s = np.std(smooth(data.T).T, axis=0)/np.sqrt(repetitions)\n",
        "        if p.log_plot:\n",
        "          line = plt.semilogy(m, alpha=0.7, label=alg.name,\n",
        "                              color=clr, ls=ls, lw=3)[0]\n",
        "        else:\n",
        "          line = plt.plot(m, alpha=0.7, label=alg.name,\n",
        "                          color=clr, ls=ls, lw=3)[0]\n",
        "          plt.fill_between(range(len(m)), m + s, m - s,\n",
        "                           color=line.get_color(), alpha=0.2)\n",
        "      if p.opt_values is not None:\n",
        "        plt.plot(p.opt_values[current_algs[0].name][0], ':', alpha=0.5,\n",
        "                 label='optimal')\n",
        "\n",
        "      ax.set_facecolor('white')\n",
        "      ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n",
        "                     labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
        "      ax.spines[\"top\"].set_visible(False)\n",
        "      ax.spines[\"bottom\"].set(visible=True, color='black', lw=1)\n",
        "      ax.spines[\"right\"].set_visible(False)\n",
        "      ax.spines[\"left\"].set(visible=True, color='black', lw=1)\n",
        "      ax.get_xaxis().tick_bottom()\n",
        "      ax.get_yaxis().tick_left()\n",
        "\n",
        "      data = np.array([smooth(np.mean(d, axis=0)) for d in p.data.values()])\n",
        "      \n",
        "      if p.log_plot:\n",
        "        start, end = calculate_lims(data, p.log_plot)\n",
        "        start = np.floor(np.log10(start))\n",
        "        end = np.ceil(np.log10(end))\n",
        "        ticks = [_*10**__\n",
        "                 for _ in [1., 2., 3., 5.]\n",
        "                 for __ in [-2., -1., 0.]]\n",
        "        labels = [r'${:1.2f}$'.format(_*10** __)\n",
        "                  for _ in [1, 2, 3, 5]\n",
        "                  for __ in [-2, -1, 0]]\n",
        "        plt.yticks(ticks, labels)\n",
        "      plt.ylim(calculate_lims(data, p.log_plot))\n",
        "      plt.locator_params(axis='x', nbins=4)\n",
        "      \n",
        "      plt.title(p.title)\n",
        "      if i == len(plot_data) - 1:\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "def run_experiment(bandit_constructor, algs, repetitions, number_of_steps):\n",
        "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
        "  reward_dict = {}\n",
        "  regret_dict = {}\n",
        "  optimal_value_dict = {}\n",
        "\n",
        "  for alg in algs:\n",
        "    reward_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    regret_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    optimal_value_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "\n",
        "    for _rep in range(repetitions):\n",
        "      bandit = bandit_constructor()\n",
        "      alg.reset()\n",
        "\n",
        "      action = None\n",
        "      reward = None\n",
        "      for _step in range(number_of_steps):\n",
        "        action = alg.step(action, reward)\n",
        "        reward = bandit.step(action)\n",
        "        regret = bandit.regret(action)\n",
        "        optimal_value = bandit.optimal_value()\n",
        "\n",
        "        reward_dict[alg.name][_rep, _step] = reward\n",
        "        regret_dict[alg.name][_rep, _step] = regret\n",
        "        optimal_value_dict[alg.name][_rep, _step] = optimal_value\n",
        "\n",
        "  return reward_dict, regret_dict, optimal_value_dict\n",
        "\n",
        "\n",
        "def train_agents(agents, number_of_arms, number_of_steps, repetitions=100,\n",
        "                 success_reward=1., fail_reward=0.,\n",
        "                 bandit_class=BernoulliBandit):\n",
        "\n",
        "  success_probabilities = np.arange(0.3, 0.7 + 1e-6, 0.4/(number_of_arms - 1))\n",
        "\n",
        "  bandit_constructor = partial(bandit_class,\n",
        "                               success_probabilities=success_probabilities,\n",
        "                               success_reward=success_reward,\n",
        "                               fail_reward=fail_reward)\n",
        "  rewards, regrets, opt_values = run_experiment(\n",
        "      bandit_constructor, agents, repetitions, number_of_steps)\n",
        "\n",
        "  smoothed_rewards = {}\n",
        "  for agent, rs in rewards.items():\n",
        "    smoothed_rewards[agent] = np.array(rs)\n",
        "\n",
        "  PlotData = collections.namedtuple('PlotData',\n",
        "                                    ['title', 'data', 'opt_values', 'log_plot'])\n",
        "  total_regrets = dict([(k, np.cumsum(v, axis=1)) for k, v in regrets.items()])\n",
        "  plot_data = [\n",
        "      PlotData(title='Smoothed rewards', data=smoothed_rewards,\n",
        "               opt_values=opt_values, log_plot=False),\n",
        "      PlotData(title='Current Regret', data=regrets, opt_values=None,\n",
        "               log_plot=True),\n",
        "      PlotData(title='Total Regret', data=total_regrets, opt_values=None,\n",
        "               log_plot=False),\n",
        "  ]\n",
        "\n",
        "  plot(agents, plot_data, repetitions)\n",
        "\n",
        "def calculate_lims(data, log_plot=False):\n",
        "  y_min = np.min(data)\n",
        "  y_max = np.max(data)\n",
        "  diff = y_max - y_min\n",
        "  if log_plot:\n",
        "    y_min = 0.9*y_min\n",
        "    y_max = 1.1*y_max\n",
        "  else:\n",
        "    y_min = y_min - 0.05*diff\n",
        "    y_max = y_max + 0.05*diff\n",
        "  return y_min, y_max\n",
        "\n",
        "def argmax(array):\n",
        "  \"\"\"Returns the maximal element, breaking ties randomly.\"\"\"\n",
        "  return np.random.choice(np.flatnonzero(array == array.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# A) Agent implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBHsuFyapu5r"
      },
      "source": [
        "\n",
        "All agents should be in pure Python/NumPy.\n",
        "\n",
        "You cannot use any AutoDiff packages (Jax, TF, PyTorch, etc.)\n",
        "\n",
        "Each agent, should implement the following methods:\n",
        "\n",
        "**`step(self, previous_action, reward)`:**\n",
        "\n",
        "Should update the statistics by updating the value for the previous_action towards the observed reward.\n",
        "\n",
        "(Note: make sure this can handle the case that previous_action=None, in which case no statistics should be updated.)\n",
        "\n",
        "(Hint: you can split this into two steps: 1. update values, 2. get new action.  Make sure you update the values before selecting a new action.)\n",
        "\n",
        "**`reset(self)`:**\n",
        "\n",
        "Resets statistics (should be equivalent to constructing a new agent from scratch).\n",
        "\n",
        "Make sure that the initial values (after a reset) are all zero.\n",
        "\n",
        "**`__init__(self, name, number_of_arms, *args)`:**\n",
        "\n",
        "The `__init__` should take at least an argument `number_of_arms`, and (potentially) agent specific args."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwUUBXgQ2MCk"
      },
      "source": [
        "## Example agent\n",
        "\n",
        "The following code block contains an example random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPYlY9M22JOI"
      },
      "outputs": [],
      "source": [
        "class Random(object):\n",
        "  \"\"\"A random agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms', uniformly at\n",
        "  random. The 'previous_action' argument of 'step' is ignored.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, number_of_arms):\n",
        "    \"\"\"Initialise the agent.\n",
        "    \n",
        "    Sets the name to `random`, and stores the number of arms. (In multi-armed\n",
        "    bandits `arm` is just another word for `action`.)\n",
        "    \"\"\"\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = name\n",
        "\n",
        "  def step(self, unused_previous_action, unused_reward):\n",
        "    \"\"\"Returns a random action.\n",
        "    \n",
        "    The inputs are ignored, but this function still requires an action and a\n",
        "    reward, to have the same interface as other agents who may use these inputs\n",
        "    to learn.\n",
        "    \"\"\"\n",
        "    return np.random.randint(self._number_of_arms)\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDTyvlZsvSQq"
      },
      "source": [
        "\n",
        "## Q1 [2 pts]\n",
        "Implement a UCB agent.\n",
        "\n",
        "The `bonus_multiplier` is the parameter $c$ from the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM5NtZ3Q2X0F"
      },
      "outputs": [],
      "source": [
        "class UCB(object):\n",
        "  def __init__(self, name, number_of_arms, bonus_multiplier):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._bonus_multiplier = bonus_multiplier\n",
        "    self.name = name\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    ...\n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqJxDegZtqXN"
      },
      "source": [
        "## Q2 [1 pt]\n",
        "Implement an $\\epsilon$-greedy agent.\n",
        "\n",
        "This agent should be able to support time-changing $\\epsilon$ schedules.\n",
        "\n",
        "Thus, your agent should accept both constants and callables as constructor argument `epsilon`; callables are used to decay the $\\epsilon$ parameter over time, for instance according to a polynomial schedule: $\\epsilon_t = t^{-\\eta}$ with $\\eta \\in [0, 1]$).\n",
        "\n",
        "\n",
        "If multiple actions have the same value, ties should be broken randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_1pB2p7146i"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedy(object):\n",
        "  \"\"\"An epsilon-greedy agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms'; with probability\n",
        "  `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
        "  with probability `epsilon` it samples an action uniformly at random.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, number_of_arms, epsilon=0.1):\n",
        "    self.name = name\n",
        "    ...\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    \"\"\"Update the learnt statistics and return an action.\n",
        "\n",
        "    A single call to step uses the provided reward to update the value of the\n",
        "    taken action (which is also provided as an input), and returns an action.\n",
        "    The action is either uniformly random (with probability epsilon), or greedy\n",
        "    (with probability 1 - epsilon).\n",
        "\n",
        "    If the input action is None (typically on the first call to step), then no\n",
        "    statistics are updated, but an action is still returned.\n",
        "    \"\"\"\n",
        "    ...\n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKI7uNjI1Ym"
      },
      "source": [
        "## Q3 [2 pts]\n",
        "Implement a REINFORCE agent.\n",
        "\n",
        "While `softmax` distributions are a common parametrization for policies over discrete action-spaces, they are not the only choice. In this exercise we ask you to implement REINFORCE with the `square-max` policy parameterization. With this parametrisation the probabilities depend on the action preferences $p(\\cdot)$ according to the expression:\n",
        "\n",
        "$$\\pi(a) = \\frac{p(a)^2}{\\sum_b p(b)^2}\\,.$$\n",
        "\n",
        "Implement a REINFORCE policy-gradient method for updating the preferences under this policy distribution. The action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n",
        "\n",
        "The agent should be able to use a baseline or not (as defined in the constructor). The `step_size` parameter $\\alpha$ used to update the policy must also be configurable in the constructor.\n",
        "\n",
        "The baseline should track the average reward so far, using the same `step_size` used to update the policy.\n",
        "\n",
        "The `step_size` and whether or not a baseline is used are defined in the constructor by feeding additional arguments in place of `...` below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqcC-OZq9bP7"
      },
      "outputs": [],
      "source": [
        "class REINFORCE(object):\n",
        "  def __init__(self, name, number_of_arms, step_size=0.1, baseline=False):\n",
        "    self.name = name\n",
        "    ...\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    ...\n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZsPzCmDxAh"
      },
      "source": [
        "# B) Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkk8sMxE0N4"
      },
      "source": [
        "**Run the cell below to train the agents and generate the plots for the first experiment.**\n",
        "\n",
        "Trains the agents on a Bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 1, and a reward on failure of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06P4AfZw1GSH"
      },
      "source": [
        "## Experiment 1: Bernoulli bandit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loe_YN7Tv8yY"
      },
      "outputs": [],
      "source": [
        "%%capture experiment1\n",
        "\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "agents = [\n",
        "    Random(\n",
        "        \"random\",\n",
        "        number_of_arms),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.1),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/t$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t**0.5),\n",
        "    UCB(\"UCB\",\n",
        "        number_of_arms,\n",
        "        bonus_multiplier=1/np.sqrt(2)),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=False),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=True),\n",
        "]\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnbpeszVp04-"
      },
      "outputs": [],
      "source": [
        "experiment1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyaMm-sjz9a"
      },
      "source": [
        "## Q4 [4 pts total]\n",
        "(Answer inline in the markdown below each question, **within this text cell**.)\n",
        "\n",
        "**[2 pts]**\n",
        "For each algorithm in the plots above, explain whether or not we should be expected it to be good in general, in terms of total regret.\n",
        "\n",
        "*Answer here.*\n",
        "\n",
        "**[2 pts]** Explain the relative ranking of the $\\epsilon$-greedy algorithms in this experiment.\n",
        "\n",
        "*Answer here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YO5NDaPGDsp"
      },
      "source": [
        "## Experiment 2: reward = 0 on success, reward = -1 on failure.\n",
        "\n",
        "**Run the cell below to train the agents and generate the plots for the second experiment.**\n",
        "Reruns experiment 1 but on a different bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 0, and a reward on failure of -1.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "7cvJf4WzmJXK"
      },
      "outputs": [],
      "source": [
        "%%capture experiment2\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             success_reward=0., fail_reward=-1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5RXnvnFLOGa"
      },
      "outputs": [],
      "source": [
        "experiment2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GOe5RDsnj4J"
      },
      "source": [
        "## Q5 [2 pts]\n",
        "For each algorithm, note whether the performance changed significantly compared to the **experiment 1**, and explain why it did or did not.\n",
        "\n",
        "(Use at most two sentences per algorithm).\n",
        "\n",
        "*Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRDmw4nyFI73"
      },
      "source": [
        "## Run the following cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drtsr8Cc1OWl"
      },
      "source": [
        "## Experiment 3: Non-stationary bandit\n",
        " * Reward on `failure` changes from 0 to +2.\n",
        " * Reward on `success` remains at +1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4RseDt-MCkq"
      },
      "outputs": [],
      "source": [
        "%%capture experiment3\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "agents = [\n",
        "    Random(\n",
        "        \"random\",\n",
        "        number_of_arms),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.1),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t**0.5),\n",
        "    UCB(\"UCB\",\n",
        "        number_of_arms,\n",
        "        bonus_multiplier=1/np.sqrt(2)),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=True),\n",
        "\n",
        "]\n",
        "\n",
        "roving_bandit_class = partial(NonStationaryBandit, change_is_good=True)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sErI1V9h1ScE"
      },
      "source": [
        "## Experiment 4: Non-stationary bandit\n",
        " * Reward on `failure` changes from 0 to +1.\n",
        " * Reward on `success` changes from +1 to 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT_mZxCIAfg9"
      },
      "outputs": [],
      "source": [
        "%%capture experiment4\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "\n",
        "\n",
        "roving_bandit_class = partial(NonStationaryBandit, change_is_good=False)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aWjHNwbEsDJ"
      },
      "outputs": [],
      "source": [
        "experiment3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s703_VCICCTL"
      },
      "outputs": [],
      "source": [
        "experiment4.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x84zO7DK2_t"
      },
      "source": [
        "## Q6 [9 pts total]\n",
        "\n",
        "Observe the reward and regret curves above.  After 800 steps, the rewards change. In **experiment 3** `success` continues to yield a reward of +1, but `failure` changes from a reward of 0 to a reward of +2.  In **experiment 4**, `success` is now worth 0 and `failure` is worth +1.\n",
        "\n",
        "Below, we ask for explanations.  Answer each question briefly, using at most three sentences per question.\n",
        "\n",
        "**[2 pts]** In **experiment 3** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "**[2 pts]** In **experiment 4** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "**[2 pts]** Explain how and why the current-regret curve for UCB in **experiment 3** differs from the curve in **experiment 4**.\n",
        "\n",
        "> *Answer here*\n",
        "\n",
        "**[3 pts]** In general, if rewards can be non-stationary, and we don't know the exact nature of the non-stationarity, how could we modify UCB to perform better?   Be specific and concise.\n",
        "\n",
        "> *Answer here*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGhirrkCChAZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Coursework, part I.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}